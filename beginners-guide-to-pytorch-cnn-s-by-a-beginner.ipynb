{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A beginners guide to Convolutional Neural Networks (CNN's) - by an enthusiastic beginner!**\n",
    "\n",
    "The following notebook is a beginners guide to creating a Convolutuonal Neural Network from scratch in PyTorch. It's my first kernel on Kaggle so let me know if and where I made mistakes!\n",
    "I will be using the MNIST dataset and will try to highlight the areas I found particularly hard to grasp in the hopes that it will help someone else.\n",
    "\n",
    "Many thanks to Morvan Zhou, whos PyTorch tutorial really helped me understand CNN's (https://github.com/MorvanZhou/PyTorch-Tutorial) and whos work pretty heavily influenced this notebook. Also many thanks to fastai whos free online courses got me interested in data science in the first place (https://www.fast.ai/).\n",
    "\n",
    "*Update - I didn't realise how good writing something you plan to make public is at getting you motivated to learn!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import the libraries we'll be using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn                         \n",
    "# torch.nn contains the classes we will use to actually build the CNN\n",
    "from torch.autograd import Variable           \n",
    "# for recording the operations used to create tensors, for computing\n",
    "# the gradients through backpropagation (deprecated)\n",
    "import torch.utils.data as Data               \n",
    "# for creating our dataset and our dataloader \n",
    "import torchvision                            \n",
    "# contains some datasets (such as MNIST we use here) and some common \n",
    "# transforms\n",
    "import matplotlib.pyplot as plt               \n",
    "# matplotlib for plotting %matplotlib inline means you can plot in the\n",
    "# notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Set manual_seed: sets the random seed. This had me confused for awhile. But essentially, as a random number generator is only pseudo-random, setting the manual seed means the same sequence of random numbers are generated each time. I.e. if you create 4 random numbers in a Kaggle kernel and 4 random numbers in your own Jupyter notebook, if the seed is set to the same number, they will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcaf26049f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create the training data. For this we will use torchvision.datasets. We need to specify that it is the train data, which transforms we want and also whether to download it. If you have already downloaded it onto your device, set download=False. If you are running this code in Kaggle, you will need to turn the internet on. Also, I find it helpful to check the size of anything I am working with to help get my head around what's in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:00, 25050808.34it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 290680.46it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 6849277.24it/s]                           \n",
      "8192it [00:00, 84989.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = torchvision.datasets.MNIST(root='./mnist/',train = True,transform = torchvision.transforms.ToTensor(),download = True)\n",
    "trainData.data.size()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, we are working with 60,000 images of size 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Having a look at one of the pictures. Using dataset.data[111] gives us the 111th (28, 28) tensor that is in our dataset. We then convert it to a numpy array with numpy and then plot using plt.imshow. We also grab the 111th target to use as the plots title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 28]), (28, 28))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainData.data[111].numpy(), cmap='gray')\n",
    "plt.title('%d' % trainData.targets[111])\n",
    "trainData.data[111].shape, trainData.data[111].numpy().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Creating the DataLoader. We use Data.Dataloader and pass in our trainData, set our batch size and set Shuffle to True. Batch size is the number of images out of the 60,000 that will be looked at at once and used to calculate the gradients and update the weights of our parameters. Setting shuffle = True means the batches are shuffled before each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = Data.DataLoader(dataset=trainData, batch_size=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Creating the test data. Similar to Step 3, this creates our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = torchvision.datasets.MNIST(root='./mnist/', train=False, transform = torchvision.transforms.ToTensor())\n",
    "testData.data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Reshaping the test data tensor to 4d with torch.unsqueeze. Then checking the shape. Always check the shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.unsqueeze(testData.data, dim=1).type(torch.FloatTensor)[:]/255 \n",
    "# need to convert to FloatTensor.\n",
    "test_x = Variable(test_x)\n",
    "test_x.data.size()\n",
    "\n",
    "# Can also use view\n",
    "# test_x = testData.data.view((10000, -1, 28, 28))\n",
    "# test_x.data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Creating the test targets and checking the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = testData.targets[:]\n",
    "test_y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Creating the Neural Network. This step is the most detailed, so I will comment in the code cell instead of here. But essentially we build a CNN class which inherits from nn.Module takes our data above as inputs and outputs our prediction for the image. I will try to add an intuitive explanation once I manage to get it into my own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):   # the class we create will inherit from nn.Module\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()                        \n",
    "        # we always include this line of code. Lets us use the base class functions.\n",
    "        \n",
    "        self.conv1 = nn.Sequential(                        \n",
    "        \n",
    "        # the input shape will be (1, 28, 28)\n",
    "        nn.Conv2d(1, 8, 3, 1, 1),                   \n",
    "        # the output shape will be (8, 28, 28)\n",
    "            \n",
    "        # The Conv2d layer above takes 5 numbers in this case. (input_channels=1, \n",
    "        # output_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # So the input image is 1 channel x 28 x 28 pixels. Padding=1 means we add \n",
    "        # a row/column of zeros to every side to prevent the output\n",
    "        # from losing a row/column as the kernel is 3x3. If the kernel was 5x5 we \n",
    "        # would need to increase padding to 2. \n",
    "        # Stride of 1 means we move the kernel over by one pixel at a time. \n",
    "        # If we wanted to half the output to 14 x 14 we could set stride to\n",
    "        # 2. Lastly, 8 output channels means we pass 8 convolutional kernels \n",
    "        # over the image to end up with 8 x (1 x 28 x 28) -> (8 x 28 x 28)\n",
    "           \n",
    "        # ReLU and BatchNorm: I will try to add a better explanation in the future :-)\n",
    "            \n",
    "        nn.ReLU(),\n",
    "            \n",
    "        # A ReLU (Rectified Linear Unit) is a non-linear activation function that, in \n",
    "        # simple terms, just throws away the negatives. It can\n",
    "        # be represented as f(x) = max(0, x).\n",
    "            \n",
    "        nn.BatchNorm2d(8),\n",
    "            \n",
    "        # BatchNorm (from what I understand) normalises the activations of the layer \n",
    "        # by transforming them to have 0 mean and unit variance\n",
    "        # (i.e. 0 mean SD of 1). The issue is that each batch that passes through \n",
    "        # the network may not have the same distributions.\n",
    "        # Normalising the activations is meant to help the network train faster by \n",
    "        # giving it more uniform activation distributions.\n",
    "        # Alternatively to the above, BatchNorm makes models train faster as it smooths \n",
    "        # the optimisation landscape (that crazy graph people\n",
    "        # show you when they talk about gradient descent). Instead of an irregular \n",
    "        # graph, with many local minima and bumps, BatchNorm \n",
    "        # gives us a smooth graph. So This means when you train, the gradients are \n",
    "        # more predictable and stable.\n",
    "        )\n",
    "        \n",
    "        # I have 4 convolutional layers in this CNN. Not really sure how to choose\n",
    "        # the structure but this one seems to work okay. If anyone has any ideasd\n",
    "        # on how to decide on a structure let me know.\n",
    "        \n",
    "        self.conv2 = nn.Sequential(     # the input shape will bee (8, 28, 28)\n",
    "        nn.Conv2d(8, 16, 3, 1, 1),      # the output shape will be (16, 28, 28)\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16),             # needs to match output channels above\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(     # the input shape will be (16, 28, 28)\n",
    "        nn.Conv2d(16, 32, 3, 1, 1),     # the output shape will be (32, 28, 28)\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),                       \n",
    "        nn.MaxPool2d(2)                 \n",
    "        # the max pool output is (32, 14, 14). Max pooling takes the max value of a\n",
    "        # square of cells and throws the rest away. This effectively means the width and\n",
    "        # height of the shape is halved in this case as our max pool is 2 x 2.\n",
    "        )\n",
    "        self.conv4 = nn.Sequential( # the input shape will be (32, 14, 14)\n",
    "        nn.Conv2d(32, 32, 3, 1, 1),    # the output shape will be (32, 14, 14)\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 14 * 14, 10)          \n",
    "        # this takes the conv4 output from 6272 features down to 10 for our 10 digits\n",
    "\n",
    "    # the forward function basically defines how our network works. It takes the CNN above\n",
    "    # and each mini batch x, and passes it through the series of functions we defined above.\n",
    "    # It then returns the output function (our predicted digit). If you prefer, you could\n",
    "    # have your ReLU and BatchNorm here instead of above like x = ReLU(self.conv1(x)).\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # We have to reshape our conv4 output to fit it into the linear layer.\n",
    "        output = self.out(x)\n",
    "        return output, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)Step 10: Creating an instance of our CNN class. Here I create an instance of the CNN class we created above called mnist_cnn. This is our model we will be using. You can then print the model to see a summary with print(mnist_cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (out): Linear(in_features=6272, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn = CNN()\n",
    "print(mnist_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: Defining our loss function. Here I define the loss function that we are trying to minimise in training. For this problem I have used Cross Entropy Loss. This loss function is also called log loss. Our model outputs 10 probabilities, of which we take the max to find our prediction. With log loss, our loss is small if a high probability is given for the actual label, and it increases rapidly as our predicted probabilty of the actual label decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossF = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12: Defining epochs. An epoch is how many times we look at the complete dataset during training. If set to 1, we pull out mini batches in training until we have seen all of the data. If two, we do it again etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13: Here I have defined a function TrainCNN to train my model. The function iterates through the dataset using the dataloader, passes each minibatch through the network, works out the loss and updates the parameters. It then prints out metrics every 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainCNN(dataLoader, model, num_epochs, loss_function, lr):\n",
    "    for i in range(num_epochs):\n",
    "        # Enumerate took me awhile to get my head around. But it basically returns a counter\n",
    "        # with each of the mini batches from my dataloader of the size I set as batch size. \n",
    "        # I.e. it will return -> 1, (images minibatch 1, labels minibatch 1),\n",
    "        # -> 2, (images minibatch 2, labels minibatch 2) until we have gone through all of\n",
    "        # the data in the dataloader.\n",
    "        for step, (images,labels) in enumerate(dataLoader):\n",
    "            images_x = Variable(images)\n",
    "            labels_y = Variable(labels)\n",
    "\n",
    "            output = model(images_x)[0]\n",
    "            # output is the 10 predictions from our CNN.\n",
    "            loss = loss_function(output, labels_y)\n",
    "            # loss is the loss of our predictions vs actual.\n",
    "            optimiser = torch.optim.Adam(model.parameters(), lr)\n",
    "            # Creating the optimiser and feeding in the parameters and learning rate.\n",
    "            optimiser.zero_grad()\n",
    "            # This zeroes out the gradients as they would accumulate otherwise.\n",
    "            loss.backward()\n",
    "            # This computes the derivatives of the parameters of the model.\n",
    "            optimiser.step()\n",
    "            # Updates the parameters of the model by adding the learning rate multiplies\n",
    "            # by the derivative of each parameter.\n",
    "\n",
    "            if step % 500 == 0: # Only does this every 500 minibatches.\n",
    "                test_output, last_layer = model(test_x) \n",
    "                # Gets our 10 predictions of each x in the nth minibatch\n",
    "                pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                # Finds the maximum probability and records as prediction for all.\n",
    "                accuracy = (pred_y == test_y).sum().item() / float(test_y.size(0))\n",
    "                # Compares to the actual to work out how many percent are correct.\n",
    "                print('Epoch: '+ str(i) + '| test accuracy: %.4f' % accuracy)\n",
    "                # Prints an accuracy to 4 dp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14: This is the final step and it calls out TrainCNN function 3 times at 3 different learning rates. I usually start at 1e-3 and decrease by a factor of 10, this for loop just does that for me. You can carry on a but more if you want, but it doesn't seem to get better for me once I get to 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| test accuracy: 0.3443\n",
      "Epoch: 0| test accuracy: 0.9752\n",
      "Epoch: 0| test accuracy: 0.9815\n",
      "Epoch: 0| test accuracy: 0.9817\n",
      "Epoch: 0| test accuracy: 0.9803\n",
      "Epoch: 0| test accuracy: 0.9844\n",
      "Epoch: 0| test accuracy: 0.9881\n",
      "Epoch: 0| test accuracy: 0.9879\n",
      "Epoch: 0| test accuracy: 0.9876\n",
      "Epoch: 0| test accuracy: 0.9890\n",
      "Epoch: 0| test accuracy: 0.9894\n",
      "Epoch: 0| test accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    lr = 1e-2/(i*10)\n",
    "    # Decreases the lr by factor of 10 each iteration.\n",
    "    TrainCNN(trainLoader, mnist_cnn,1,lossF, lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Looks pretty good for a simple model. Accuracy of 98-99%. Please comment and let me know where I have messed up. Or if anyone has any suggestions to how to improve the accuracy, please share!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
